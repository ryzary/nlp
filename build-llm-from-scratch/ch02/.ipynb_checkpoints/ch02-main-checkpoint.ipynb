{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51867f3f-7e61-450e-b15b-fa7005337545",
   "metadata": {},
   "source": [
    "## Ch.2 Working with Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b9227-b46e-48bb-a62b-1f5d2fdb49ac",
   "metadata": {},
   "source": [
    "- The full notebook can be accessed via: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/01_main-chapter-code/ch02.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6260b4e-1eb8-4e7f-bd19-ab49065cbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bb8b3a-ca24-4adf-8dd6-ad16b35a13ac",
   "metadata": {},
   "source": [
    "### 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b773e94d-6603-412e-a557-c5d70f4615b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total num of character:  20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total num of character: \", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7a511b-0faf-4b98-b714-876876ddbecd",
   "metadata": {},
   "source": [
    "### 2. Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9d7930-0dac-414f-b768-61c35a07f47f",
   "metadata": {},
   "source": [
    "- A vocabulary is the set of all possible tokens that the model can recognize and process.\n",
    "- Let's construct a vocabulary by creating a dictionary with unique tokens as the keys and Token IDs as the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80831afc-131d-4446-8066-f5f91bbb5dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e76d031-fd58-4dd6-b67c-8c65e822b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  1130\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary\n",
    "all_words = sorted(set(preprocessed)) # sorted list of unique words\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(\"Vocab size: \", vocab_size)\n",
    "\n",
    "vocab = {token: integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed0bb594-4959-4b23-9ea5-4c3dd6103f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('\"', 1)\n",
      "(\"'\", 2)\n",
      "('(', 3)\n",
      "(')', 4)\n",
      "(',', 5)\n"
     ]
    }
   ],
   "source": [
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 5:\n",
    "        break # print only the first 5 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08a2b43-1550-4f38-b6d0-f44c805f36a8",
   "metadata": {},
   "source": [
    "### 3. SimpleTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "983d3ed4-a9fd-4286-9255-2ab71327311f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        preprocessed = [\n",
    "            item if item in self.str_to_int \n",
    "            else \"<|unk|>\" for item in preprocessed # <|unk|> is for unknown token\n",
    "        ]\n",
    "\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "        \n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "492ff9a0-d882-4dbd-bde8-781df1170175",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7cf7d5e7-1e62-4170-8a81-de5f1cc0cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\"It's the last he painted, you know,\" \n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecae719-8a9e-43ad-af2b-8e3fbcc39e20",
   "metadata": {},
   "source": [
    "## 4. BytePair Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37bd64c-3176-4259-b10a-c9a6e08c3a53",
   "metadata": {},
   "source": [
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- It allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- In this chapter, we are using the BPE tokenizer from OpenAI's open-source tiktoken library, which implements its core algorithms in Rust to improve computational performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c79c6d1-4819-4a15-a46f-af42dd5949f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04af209e-bb60-4487-b8eb-2343425f4877",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ca89749-c357-46a4-abe4-8ce187ff1279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e8aaa9-c62c-49cb-aa40-6e20feeadaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289c52b1-5e31-4ecf-9491-75c5a85bf2d4",
   "metadata": {},
   "source": [
    "## 5. Data sampling with a sliding windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e224f36-f26a-4cdb-a40d-74a622cf9340",
   "metadata": {},
   "source": [
    "We train LLMs to generate one word at a time, so we want to prepare the training data accordingly where the next word in a sequence represents the target to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e981298-ae3c-4a15-bb68-1eeafb7db54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# let's encode the previous text\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee32131-ca68-4735-a5ba-cf8cbf6f2ced",
   "metadata": {},
   "source": [
    "Let's implement a simple data loader that iterates over the input dataset and returns the inputs and targets shifted by one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feebe730-07c3-4ef6-bf83-b30755b18eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067d2bfa-ab09-442b-bcfd-511b0cbd3134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "        assert len(token_ids) > max_length, \"Number of tokenized inputs must at least be equal to max_length+1\"\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d8e38de-e400-4dcf-b1d3-6b71b65040e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93ddb5-5b0d-4eae-a592-2638fcd32600",
   "metadata": {},
   "source": [
    "Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4.\n",
    "- Context size: context size (also known as context window or context length) refers to the maximum number of tokens the model can consider at once when generating or analyzing text.\n",
    "- The stride setting dictates the number of positions the inputs shift across batches, emulating a sliding window\n",
    "approach. Try changing the stride from 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "01d0fa9a-f08c-4e65-9de5-adeb37548859",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca98d84a-6440-4197-901d-c848e8afe425",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_dataloader_v1(txt=raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67447583-b140-4042-891f-88702e1698f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n",
      "Inputs:\n",
      " tensor([[2885, 1464, 1807, 3619]])\n",
      "\n",
      "Targets:\n",
      " tensor([[1464, 1807, 3619,  402]])\n"
     ]
    }
   ],
   "source": [
    "first_batch = next(data_iter)\n",
    "inputs, targets = first_batch\n",
    "print(first_batch)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b88bf8c9-d11c-4974-9c48-f4058f1a143e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[1464, 1807, 3619,  402]]), tensor([[1807, 3619,  402,  271]])]\n",
      "Inputs:\n",
      " tensor([[1464, 1807, 3619,  402]])\n",
      "\n",
      "Targets:\n",
      " tensor([[1807, 3619,  402,  271]])\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "inputs, targets = second_batch\n",
    "\n",
    "print(second_batch)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe9f41-7565-4a29-b0c0-1077beb83fe9",
   "metadata": {},
   "source": [
    "- We can also create batched outputs\n",
    "- Note that we increase the stride here so that we don't have overlaps between the batches, since more overlap could lead to increased overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9ffd3ec1-f6e9-415f-8a73-546c759a82b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]]), tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])]\n",
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "\n",
    "new_batch = next(data_iter)\n",
    "\n",
    "print(new_batch)\n",
    "\n",
    "inputs, targets = new_batch\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a9cb1-fab8-457a-9967-a9e0163e5f32",
   "metadata": {},
   "source": [
    "### 6. Creating Token Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b42e610-57d5-46c9-b1ba-52d9c83c98e5",
   "metadata": {},
   "source": [
    "- Lastly let us embed the tokens in a continuous vector representation using an embedding layer\n",
    "- Usually, these embedding layers are part of the LLM itself and are updated (trained) during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "232e02c5-9c44-4440-b73e-6f45e406087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we have the following four input examples with input ids 2, 3, 5, and 1 (after tokenization):\n",
    "\n",
    "input_ids = torch.tensor([2, 3, 5, 1])\n",
    "\n",
    "# For the sake of simplicity, suppose we have a small vocabulary of only 6 words and we want to create embeddings of size 3:\n",
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0bd94d95-186e-4146-9fba-7f7009c2c030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.1690],\n",
      "        [ 0.9178,  1.5810,  1.3010],\n",
      "        [ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-1.1589,  0.3255, -0.6315],\n",
      "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc5a2c9c-1764-49b0-aeb2-7c61cee7c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# To convert a token with id 3 into a 3-dimensional vector, we do the following:\n",
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ead77c8-1bc4-479f-ba36-945f1e651b7b",
   "metadata": {},
   "source": [
    "- Note that the above is the 4th row in the embedding_layer weight matrix\n",
    "- To embed all four input_ids values above, we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b97c6f6-0d73-47ee-aa7f-61edd67a1546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2753, -0.2010, -0.1606],\n",
      "        [-0.4015,  0.9666, -1.1481],\n",
      "        [-2.8400, -0.7849, -1.4096],\n",
      "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(embedding_layer(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b297f495-3da7-4261-b424-9007219dbb1e",
   "metadata": {},
   "source": [
    "## 7. Encoding word positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163318b-c4f6-4b31-9b7f-1deeb63cc749",
   "metadata": {},
   "source": [
    "- In the previous section, we converted the token IDs into a continuous vector\n",
    "representation, the so-called token embeddings. In principle, this is a suitable\n",
    "input for an LLM.\n",
    "-  However, a minor shortcoming of LLMs is that their self-\n",
    "attention mechanism, which will be covered in detail in chapter 3, doesn't\n",
    "have a notion of position or order for the tokens within a sequence.\n",
    "- The way the previously introduced embedding layer works is that the same\n",
    "token ID always gets mapped to the same vector representation, regardless of\n",
    "where the token ID is positioned in the input sequence,\n",
    "- In principle, the deterministic, position-independent embedding of the token\n",
    "ID is good for reproducibility purposes. However, since the self-attention\n",
    "mechanism of LLMs itself is also position-agnostic, it is helpful to inject\n",
    "additional position information into the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3d47fa27-d372-41ec-bd58-bf5d36afcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The BytePair encoder has a vocabulary size of 50,257:\n",
    "# Suppose we want to encode the input tokens into a 256-dimensional vector representation:\n",
    "\n",
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955303df-1e93-42fc-9c7f-70d4500ceccb",
   "metadata": {},
   "source": [
    "- If we sample data from the dataloader, we embed the tokens in each batch into a 256-dimensional vector\n",
    "- If we have a batch size of 8 with 4 tokens each, this results in a 8 x 4 x 256 tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2151188a-e349-499c-9ce6-deb463066ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text, batch_size=8, max_length=max_length,\n",
    "    stride=max_length, shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "01bd0607-d269-4d4a-8c3b-6a406f7b9441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Inputs shape:\n",
      " torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token IDs:\\n\", inputs)\n",
    "print(\"\\nInputs shape:\\n\", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b293a77a-1391-464b-9036-2d988f596981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b3e174-3da6-4831-b28a-8e5d7263311f",
   "metadata": {},
   "source": [
    "- GPT-2 uses __absolute__ position embeddings, so we just create another embedding layer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6bb6fa2f-70a2-485f-9ef6-62197374138b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "\n",
    "# uncomment & execute the following line to see how the embedding layer weights look like\n",
    "# print(pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "76c1c7ec-25a8-4f46-b6b9-a2e31ed4f3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(pos_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10ff2e-81bb-4be0-a654-9ca51f68f1ec",
   "metadata": {},
   "source": [
    "- To create the input embeddings used in an LLM, we simply __add__ the token and the positional embeddings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5405e465-8a52-47cc-bdb0-714298ec6541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "print(input_embeddings.shape)\n",
    "\n",
    "# uncomment & execute the following line to see how the embeddings look like\n",
    "# print(input_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc59302-b88a-4f7f-81c9-5380b91ece79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
